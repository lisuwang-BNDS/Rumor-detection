[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file tokenizer.json
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file tokenizer.model
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file added_tokens.json
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file special_tokens_map.json
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file tokenizer_config.json
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file chat_template.jinja
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2337 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-10-18 05:06:22] configuration_utils.py:763 >> loading configuration file /root/autodl-fs/unsloth/GLM-4-9B-0414/config.json
[INFO|2025-10-18 05:06:22] configuration_utils.py:839 >> Model config Glm4Config {
  "architectures": [
    "Glm4ForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 13696,
  "max_position_embeddings": 32768,
  "model_type": "glm4",
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_key_value_heads": 2,
  "pad_token_id": 151330,
  "partial_rotary_factor": 0.5,
  "rms_norm_eps": 1e-05,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "unsloth_fixed": true,
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file tokenizer.json
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file tokenizer.model
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file added_tokens.json
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file special_tokens_map.json
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file tokenizer_config.json
[INFO|2025-10-18 05:06:22] tokenization_utils_base.py:2066 >> loading file chat_template.jinja
[INFO|2025-10-18 05:06:23] tokenization_utils_base.py:2337 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-10-18 05:06:23] logging.py:143 >> Add <|user|>,<|observation|> to stop words.
[INFO|2025-10-18 05:06:23] logging.py:143 >> Loading dataset web_search_std_rag_testdata_weibocovid_openai.json...
[INFO|2025-10-18 05:06:29] configuration_utils.py:763 >> loading configuration file /root/autodl-fs/unsloth/GLM-4-9B-0414/config.json
[INFO|2025-10-18 05:06:29] configuration_utils.py:839 >> Model config Glm4Config {
  "architectures": [
    "Glm4ForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "dtype": "bfloat16",
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 13696,
  "max_position_embeddings": 32768,
  "model_type": "glm4",
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_key_value_heads": 2,
  "pad_token_id": 151330,
  "partial_rotary_factor": 0.5,
  "rms_norm_eps": 1e-05,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.56.2",
  "unsloth_fixed": true,
  "use_cache": true,
  "vocab_size": 151552
}

[WARNING|2025-10-18 05:06:29] logging.py:328 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|2025-10-18 05:06:29] logging.py:143 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|2025-10-18 05:06:29] logging.py:143 >> KV cache is enabled for faster generation.
[WARNING|2025-10-18 05:06:29] logging.py:328 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|2025-10-18 05:06:30] modeling_utils.py:1277 >> loading weights file /root/autodl-fs/unsloth/GLM-4-9B-0414/model.safetensors.index.json
[INFO|2025-10-18 05:06:30] modeling_utils.py:2466 >> Instantiating Glm4ForCausalLM model under default dtype torch.bfloat16.
[INFO|2025-10-18 05:06:30] configuration_utils.py:1055 >> Generate config GenerationConfig {
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "pad_token_id": 151330
}

[INFO|2025-10-18 05:07:39] modeling_utils.py:5724 >> All model checkpoint weights were used when initializing Glm4ForCausalLM.

[INFO|2025-10-18 05:07:39] modeling_utils.py:5732 >> All the weights of Glm4ForCausalLM were initialized from the model checkpoint at /root/autodl-fs/unsloth/GLM-4-9B-0414.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Glm4ForCausalLM for predictions without further training.
[INFO|2025-10-18 05:07:39] configuration_utils.py:1008 >> loading configuration file /root/autodl-fs/unsloth/GLM-4-9B-0414/generation_config.json
[INFO|2025-10-18 05:07:39] configuration_utils.py:1055 >> Generate config GenerationConfig {
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "pad_token_id": 151329
}

[INFO|2025-10-18 05:07:39] logging.py:143 >> all params: 9,400,279,040
[WARNING|2025-10-18 05:07:39] logging.py:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
[INFO|2025-10-18 05:07:39] trainer.py:4623 >> 
***** Running Prediction *****
[INFO|2025-10-18 05:07:39] trainer.py:4625 >>   Num examples = 411
[INFO|2025-10-18 05:07:39] trainer.py:4628 >>   Batch size = 2
[INFO|2025-10-18 05:19:00] logging.py:143 >> Saving prediction results to saves/GLM-4-0414-9B-Chat/lora/eval_2025-10-18-05-02-25/generated_predictions.jsonl
